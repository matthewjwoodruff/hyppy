Having fixed the memory leak in wfg2, I can get a realistic comparison
with the old wfg2.

From several runs, here are timings for random/13d/ran.95pts.13d.20:

new:

hv(1) = 79457954259.3537597656
Time: 3.066666 (s)
hv(2) = 113223068180.6866912842
Time: 5.746666 (s)
hv(3) = 623880974688.9278564453
Time: 1.306667 (s)
hv(4) = 97713448855.6325836182
Time: 16.579998 (s)
hv(5) = 103277556272.6793060303
Time: 9.896666 (s)

old:

hv(1) = 79457954259.3537597656
Time: 3.053333 (s)
hv(2) = 113223068180.6866912842
Time: 5.733333 (s)
hv(3) = 623880974688.9278564453
Time: 1.303333 (s)
hv(4) = 97713448855.6325836182
Time: 16.519999 (s)
hv(5) = 103277556272.6793060303
Time: 9.889999 (s)
hv(6) = 50595619461.6716079712
Time: 14.646665 (s)
hv(7) = 75932932211.9442901611
Time: 10.669999 (s)
hv(8) = 46496151618.2195816040
Time: 34.946663 (s)
hv(9) = 70197029961.7542114258
Time: 13.879998 (s)
hv(10) = 77653999620.1856079102
Time: 13.123332 (s)
hv(11) = 63317332120.9426956177
Time: 9.746666 (s)
hv(12) = 290243235887.0772705078
Time: 2.876667 (s)

0.013333000000000261 s
0.013333000000000261 s
0.003333999999999948 s
0.059999000000001246 s
0.0066670000000002005 s

The differences for the first 5 are pretty consistent.  The new
implementation with memory leaks fixed takes about a hundredth of a
second longer, usually.  Interestingly, multiple runs of the 3obj set
turn up the same number repeatedly: 0.01333 extra seconds.  So it
seems that there's something not size dependent going on here.  I'm
not going to inspect it to death, but my guess is that there's a fixed
time overhead to all that memory allocation.  Now, I suspect that in
the long term, my Python approach will be a win.
1) Mostly-fixed memory leaks mean that we don't risk exhausting memory
with really big files / long runs.
2) Streaming approach means we don't have to hold big files in memory,
ever.
3) Streaming approach means we don't have to invoke wfg2 multiple
times when Python is directed to process multiple files.  This is
maybe the biggest win?
4) I can save a *lot* of arithmetic if I get rid of the reference
point by translating the whole set so that the reference point is at
zero.
5) By focusing on wfg2, which seems to be the best one, I can make a
really clean implementation.  Or I could at least focus down on 1 and
2, which I think are the same except that wfg1 doesn't call hv2.
There's definitely a performance difference there, so wfg2 is probably
worth the complexity.  Also, wfg0 is way slow by comparison, and wfg3
is still broken for some reason.

Now, I have, but I haven't, actually fixed the memory leak.  The cause
is decrementing nPoints so that when we try to clean up, we clean up
the wrong number.  This wasn't a problem for the old implementation,
because failing to free the memory wasn't much of a problem if you
were reading in reasonable sized files, allocating memory one time,
and then terminating.  I didn't have the stamina to do a real fix.
Instead all I did was have the front keep track of the original points
that were allocated to it as well as how many there were, so that I
could actually clean each one up.  And that's good enough for now.  I
really don't want to have to replicate the allocation logic to figure
out how many points to clean up.
