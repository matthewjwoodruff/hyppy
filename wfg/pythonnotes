What will this look like to C? To Python?

To C, I want to keep it simple.  I'm going to pass a flat array of
doubles, nobj, and npts.  C is just going to have to assume that we've
given it something reasonable.

To Python, I also want to keep it simple.  Calling wfg(iterable,
ref=point, maximize=list) should produce a meaningful hypervolume.

This means that I need to do some wrapping on two levels.  On the C
side, I need to write the function that takes those three arguments.
It needs to allocate a front containing the data.  It doesn't have the
information it needs to validate its inputs, so best of luck!

On the Python side, I need a function that wraps the primitive call
with the nice interface I want to present to the world.

The way Python handles arrays w.r.t C is that every length of array is
a new class.  OK, that's an acceptable overhead.
It looks like this:
from ctypes import c_double
Dub = c_double * 10
dub = Dub()
dub[4] = 12.1
anotherdub = Dub(*[1,2,3,4,5,6,7,8,9,10])

So in other words what I'll do is create that flat list in Python, or
even a generator (if they're splattable, which they are in Py3k.)  And
at the same time I'll create the type of appropriate size.

Actually, we're also going to take care of translating to zero on the
Python side.  Might be able to get the reference point code out of wfg
entirely.

# Scaling: A big huge issue we ignored?

Scaling might not actually be worse for hypervolume than for additive
metrics.  Multiplying everything together over a common denominator
means that scale doesn't really matter all that much --- each scaling
factor could just as well come from any objective.

Now, there's still a possible role to be played by epsilons.  First of
all, there's the sorting, and second, there's the possibility of using
a (perhaps?) much faster integer version.  Because epsilons say, we're
only interested in box occupancy, why not use box number relative to
the reference point's box number rather than the real number distance
to the reference point.  This will speed things up internally even if
we still use floats, because the nondom sort will be much more likely
to cull points.  (Assuming conversion from int to float is
deterministic, i.e. same int always has same float representation
resulting from a cast.  No need for math or roundtrip to be
consistent, so no error accumulation.)  And if we also have an integer
version, that *may* be even faster because we get to leave out the
FPU.  Assuming we change double to OBJECTIVE everywhere it makes sense
to do so.
